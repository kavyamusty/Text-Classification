{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC \n",
    "from scipy.spatial import distance\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score,cross_validate\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,\\\n",
    "classification_report,cohen_kappa_score,make_scorer,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the training and testing data\n",
    "df1 = pd.read_csv('training.csv')\n",
    "df2 = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': {'IRRELEVANT': 0,\n",
       "  'ARTS CULTURE ENTERTAINMENT': 1,\n",
       "  'BIOGRAPHIES PERSONALITIES PEOPLE': 2,\n",
       "  'DEFENCE': 3,\n",
       "  'DOMESTIC MARKETS': 4,\n",
       "  'FOREX MARKETS': 5,\n",
       "  'HEALTH': 6,\n",
       "  'MONEY MARKETS': 7,\n",
       "  'SCIENCE AND TECHNOLOGY': 8,\n",
       "  'SHARE LISTINGS': 9,\n",
       "  'SPORTS': 10}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One Hot Encoding to quantitatively represent the topics\n",
    "encoding = {'topic' : \n",
    "            {'IRRELEVANT' : 0,\n",
    "             'ARTS CULTURE ENTERTAINMENT':1,\n",
    "             'BIOGRAPHIES PERSONALITIES PEOPLE':2,\n",
    "             'DEFENCE' : 3, \n",
    "             'DOMESTIC MARKETS' : 4, \n",
    "             'FOREX MARKETS' : 5, \n",
    "             'HEALTH' : 6, \n",
    "             'MONEY MARKETS' : 7,\n",
    "             'SCIENCE AND TECHNOLOGY' : 8, \n",
    "             'SHARE LISTINGS' : 9, \n",
    "             'SPORTS' :10,\n",
    "             }}\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag of words\n",
    "ngram_range = (1,2)\n",
    "min_df = 10    #When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "max_df = 1.   #max_df: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
    "max_features = 1500\n",
    "#count = TfidfVectorizer()\n",
    "count= TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True,\n",
    "                        use_idf=True)\n",
    "bag_of_words = count.fit(df1['article_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing split - X and Y\n",
    "x_train = df1['article_words']\n",
    "y_train = df1['topic'].to_list()\n",
    "x_test = df2['article_words']\n",
    "y_test = df2['topic'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the testing and training\n",
    "x_train = bag_of_words.transform(x_train)\n",
    "x_test = bag_of_words.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation on selecting metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/apple/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/apple/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/apple/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/apple/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
    "        decision_function_shape='ovr', degree=3, gamma=0.1, kernel='sigmoid',\n",
    "        max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "        tol=0.001, verbose=False),\n",
    "    RFC(n_estimators=200, #the number of trees in the forest 9\n",
    "         criterion='gini', #a way to decide the attribute\n",
    "         max_depth=80, #max depth of the tree\n",
    "         min_samples_split=9, #The minimum number of samples required to split an internal node\n",
    "         min_samples_leaf=1, #The minimum number of samples required to be at a leaf node\n",
    "         max_features='sqrt', #The number of features to consider when looking for the best split:\n",
    "         max_leaf_nodes=None,\n",
    "         bootstrap=False,\n",
    "         warm_start=False), #When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest.\n",
    "    MultinomialNB(alpha=1.0, class_prior=None, fit_prior='True'),\n",
    "    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
    "                     metric_params=None, n_jobs=None, n_neighbors=11, p=2,\n",
    "                     weights='distance')\n",
    "]\n",
    "\n",
    "def cohen(y_true,y_predict): return cohen_kappa_score(y_true,y_predict)\n",
    "def accuracy(y_true,y_predict): return accuracy_score(y_true,y_predict)\n",
    "def precision(y_true,y_predict): return precision_score(y_true,y_predict,average='macro')\n",
    "def recall(y_true,y_predict): return recall_score(y_true,y_predict,average='macro')\n",
    "#def roc_auc(y_true,y_predict): return roc_auc_score(y_true,y_predict,average='macro')\n",
    "scoring = {'cohen':make_scorer(cohen),'accuracy':make_scorer(accuracy),\n",
    "           'precision':make_scorer(precision),'recall':make_scorer(recall)}\n",
    "classifier_accuracy_list = []\n",
    "for i, classifier in enumerate(classifiers):\n",
    "        # split the dataset into 5 folds; then test the classifier against each fold one by one\n",
    "        accuracies = cross_validate(classifier, x_train, y_train, cv=5, scoring=scoring)\n",
    "        #classifier_accuracy_list.append((accuracies.mean(), type(classifier).__name__))\n",
    "        classifier_accuracy_list.append((accuracies, type(classifier).__name__))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier SVC \n",
      " cohen_mean:0.6715813798798351,accuracy_mean: 0.7766303887840691,                                 precision_mean:0.7150691485941267,recall_mean:0.557924061423529\n",
      "classifier RandomForestClassifier \n",
      " cohen_mean:0.6213879156962081,accuracy_mean: 0.7510590966940207,                                 precision_mean:0.732372305476205,recall_mean:0.4034624129358716\n",
      "classifier MultinomialNB \n",
      " cohen_mean:0.6152752074072937,accuracy_mean: 0.7410603270410656,                                 precision_mean:0.5870507002421852,recall_mean:0.40425414539714444\n",
      "classifier KNeighborsClassifier \n",
      " cohen_mean:0.6450623282008181,accuracy_mean: 0.7536841582707059,                                 precision_mean:0.6887429887373889,recall_mean:0.5476950469432813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'fit_time': array([15.2449069 , 15.34216809, 15.0947392 , 15.13932395, 15.12666702]),\n",
       "   'score_time': array([16.63713312, 16.85792518, 17.07731414, 17.06618714, 16.90793872]),\n",
       "   'test_cohen': array([0.66805066, 0.68037011, 0.66861481, 0.67099632, 0.66987501]),\n",
       "   'test_accuracy': array([0.77573529, 0.78098739, 0.77327722, 0.77689873, 0.7762533 ]),\n",
       "   'test_precision': array([0.6731202 , 0.72841684, 0.69689268, 0.72329267, 0.75362335]),\n",
       "   'test_recall': array([0.53541098, 0.58114242, 0.56374538, 0.55980345, 0.54951807])},\n",
       "  'SVC'),\n",
       " ({'fit_time': array([23.03527713, 22.7660141 , 23.00905919, 22.60920095, 22.61260986]),\n",
       "   'score_time': array([0.88352299, 0.91984105, 0.86381316, 0.83330607, 0.83615112]),\n",
       "   'test_cohen': array([0.62695393, 0.61357241, 0.61211948, 0.6274649 , 0.62682885]),\n",
       "   'test_accuracy': array([0.75472689, 0.74369748, 0.74539716, 0.75685654, 0.75461741]),\n",
       "   'test_precision': array([0.70966642, 0.70601845, 0.74182318, 0.74271743, 0.76163605]),\n",
       "   'test_recall': array([0.41244585, 0.40684314, 0.38679168, 0.41509731, 0.39613409])},\n",
       "  'RandomForestClassifier'),\n",
       " ({'fit_time': array([0.02257204, 0.02051592, 0.02059793, 0.02168608, 0.021312  ]),\n",
       "   'score_time': array([0.02699423, 0.02883101, 0.02752209, 0.02698016, 0.02707005]),\n",
       "   'test_cohen': array([0.60670624, 0.61453635, 0.61300136, 0.61860236, 0.62352972]),\n",
       "   'test_accuracy': array([0.73581933, 0.73844538, 0.74013677, 0.74419831, 0.74670185]),\n",
       "   'test_precision': array([0.48983282, 0.68161573, 0.50118977, 0.58408743, 0.67852775]),\n",
       "   'test_recall': array([0.39642298, 0.41494875, 0.38703462, 0.41334461, 0.40951976])},\n",
       "  'MultinomialNB'),\n",
       " ({'fit_time': array([0.01127911, 0.00952601, 0.01044726, 0.01030302, 0.00962996]),\n",
       "   'score_time': array([3.05869222, 3.08247876, 3.02957201, 2.99135709, 3.0216713 ]),\n",
       "   'test_cohen': array([0.64721846, 0.66195098, 0.61776073, 0.65484691, 0.64353456]),\n",
       "   'test_accuracy': array([0.75630252, 0.76313025, 0.73435034, 0.76160338, 0.7530343 ]),\n",
       "   'test_precision': array([0.65308495, 0.70409202, 0.68785959, 0.6888728 , 0.70980559]),\n",
       "   'test_recall': array([0.5307356 , 0.59561387, 0.5419772 , 0.53753336, 0.53261521])},\n",
       "  'KNeighborsClassifier')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for e in classifier_accuracy_list:\n",
    "    print(f\"classifier {e[1]} \\n cohen_mean:{np.mean(e[0]['test_cohen'])},accuracy_mean: {np.mean(e[0]['test_accuracy'])},\\\n",
    "                                 precision_mean:{np.mean(e[0]['test_precision'])},recall_mean:{np.mean(e[0]['test_recall'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
